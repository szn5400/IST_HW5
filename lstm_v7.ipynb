{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3689e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051de473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'D:/Courses/IST 597/HW/HW5/')\n",
    "from data_utils import parse_imdb_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5021acfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_reviews = pickle.load(open('D:/Courses/IST 597/HW/HW5/data/length_reviews.pkl', 'rb'))\n",
    "pd.DataFrame(length_reviews, columns=['Length reviews']).hist(bins=100, color='blue');\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bc7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a val set split here\n",
    "train_dataset = tf.data.TFRecordDataset('D:/Courses/IST 597/HW/HW5/data/train.tfrecords')\n",
    "train_size = len(list(train_dataset))     #25000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(10000, seed=1234)\n",
    "mod_train_size = int(0.8 * train_size)\n",
    "val_size = int(0.2 * train_size)\n",
    "#print(mod_train_size, val_size)\n",
    "mod_train_dataset = train_dataset.take(mod_train_size) \n",
    "validation_dataset = train_dataset.skip(mod_train_size).take(val_size)\n",
    "\n",
    "#train_dataset, validation_dataset = get_dataset_partitions_tf(train_dataset, ds_size)\n",
    "\n",
    "train_dataset = mod_train_dataset.map(parse_imdb_sequence).shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.padded_batch(512, padded_shapes=([None],[],[]))\n",
    "\n",
    "validation_dataset = validation_dataset.map(parse_imdb_sequence).shuffle(buffer_size=10000)\n",
    "validation_dataset = validation_dataset.padded_batch(512, padded_shapes=([None],[],[]))\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset('D:/Courses/IST 597/HW/HW5/data/test.tfrecords')\n",
    "test_dataset = test_dataset.map(parse_imdb_sequence).shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.padded_batch(512, padded_shapes=([None],[],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e579b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the word vocabulary\n",
    "word2idx = pickle.load(open('D:/Courses/IST 597/HW/HW5/data/word2idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea4b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, embedding_size=100, cell_size=64, dense_size=128, \n",
    "                 num_classes=2, vocabulary_size=None, rnn_cell='lstm',\n",
    "                 device='cpu:0', checkpoint_directory=None):\n",
    "        ''' Define the parameterized layers used during forward-pass, the device\n",
    "            where you would like to run the computation on and the checkpoint\n",
    "            directory. Additionaly, you can also modify the default size of the \n",
    "            network.\n",
    "            \n",
    "            Args:\n",
    "                embedding_size: the size of the word embedding.\n",
    "                cell_size: RNN cell size.\n",
    "                dense_size: the size of the dense layer.\n",
    "                num_classes: the number of labels in the network.\n",
    "                vocabulary_size: the size of the word vocabulary.\n",
    "                rnn_cell: string, either 'lstm' or 'ugrnn'.\n",
    "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
    "                checkpoint_directory: the directory where you would like to save or \n",
    "                                      restore a model.\n",
    "        '''\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Weights initializer function\n",
    "        w_initializer = tf.compat.v1.keras.initializers.glorot_uniform()\n",
    "    \n",
    "        # Biases initializer function\n",
    "        b_initializer = tf.zeros_initializer()\n",
    "        \n",
    "        # Initialize weights for word embeddings \n",
    "        self.embeddings = tf.keras.layers.Embedding(vocabulary_size, embedding_size, \n",
    "                                                    embeddings_initializer=w_initializer)\n",
    "        \n",
    "        # Dense layer initialization\n",
    "        self.dense_layer = tf.keras.layers.Dense(dense_size, activation=tf.nn.relu, \n",
    "                                                 kernel_initializer=w_initializer, \n",
    "                                                 bias_initializer=b_initializer)\n",
    "        \n",
    "        # Predictions layer initialization\n",
    "        self.pred_layer = tf.keras.layers.Dense(num_classes, activation=None, \n",
    "                                                kernel_initializer=w_initializer, \n",
    "                                                bias_initializer=b_initializer)\n",
    "        \n",
    "        # Basic LSTM cell\n",
    "        if rnn_cell=='lstm':\n",
    "            self.rnn_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(cell_size)\n",
    "        # Else RNN cell\n",
    "        else:\n",
    "            self.rnn_cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(cell_size)\n",
    "            \n",
    "        # Define the device \n",
    "        self.device = device\n",
    "        \n",
    "        # Define the checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        \n",
    "    def predict(self, X, seq_length, is_training):\n",
    "        '''\n",
    "        Predicts the probability of each class, based on the input sample.\n",
    "\n",
    "        Args:\n",
    "            X: 2D tensor of shape (batch_size, time_steps).\n",
    "            seq_length: the length of each sequence in the batch.\n",
    "            is_training: Boolean. Either the network is predicting in\n",
    "                         training mode or not.\n",
    "        '''\n",
    "        \n",
    "        # Get the number of samples within a batch\n",
    "        num_samples = tf.shape(X)[0]\n",
    "\n",
    "        # Initialize LSTM cell state with zeros\n",
    "        state = self.rnn_cell.zero_state(num_samples, dtype=tf.float32)\n",
    "        \n",
    "        # Get the embedding of each word in the sequence\n",
    "        embedded_words = self.embeddings(X)\n",
    "        \n",
    "        # Unstack the embeddings\n",
    "        unstacked_embeddings = tf.unstack(embedded_words, axis=1)\n",
    "        \n",
    "        # Iterate through each timestep and append the predictions\n",
    "        outputs = []\n",
    "        for input_step in unstacked_embeddings:\n",
    "            output, state = self.rnn_cell(input_step, state)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        # Stack outputs to (batch_size, time_steps, cell_size)\n",
    "        outputs = tf.stack(outputs, axis=1)\n",
    "        \n",
    "        # Extract the output of the last time step, of each sample\n",
    "        idxs_last_output = tf.stack([tf.range(num_samples), \n",
    "                                     tf.cast(seq_length-1, tf.int32)], axis=1)\n",
    "        final_output = tf.gather_nd(outputs, idxs_last_output)\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        #dropped_output = tf.compat.v1.layers.Dropout(final_output, rate=0.3, training=is_training)\n",
    "        \n",
    "        # Pass the last cell state through a dense layer (ReLU activation)\n",
    "        dense = self.dense_layer(final_output)\n",
    "        \n",
    "        # Compute the unnormalized log probabilities\n",
    "        logits = self.pred_layer(dense)\n",
    "        return logits\n",
    "    \n",
    "    def loss_fn(self, X, y, seq_length, is_training):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        preds = self.predict(X, seq_length, is_training)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=preds)\n",
    "        return loss\n",
    "    \n",
    "    def grads_fn(self, X, y, seq_length, is_training):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(X, y, seq_length, is_training)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\" Function to restore trained model.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            # Run the model once to initialize variables\n",
    "            dummy_input = tf.constant(tf.zeros((1,1)))\n",
    "            dummy_length = tf.constant(1, shape=(1,))\n",
    "            dummy_pred = self.predict(dummy_input, dummy_length, False)\n",
    "            # Restore the variables of the model\n",
    "            saver = tf.compat.v1.train.Saver(self.variables)\n",
    "            saver.restore(tf.train.latest_checkpoint\n",
    "                          (self.checkpoint_directory))\n",
    "    \n",
    "    def save_model(self, global_step=0):\n",
    "        \"\"\" Function to save trained model.\n",
    "        \"\"\"\n",
    "        tf.compat.v1.train.Saver(self.variables).save(save_path=self.checkpoint_directory, \n",
    "                                       global_step=global_step)   \n",
    "        \n",
    "    def fit(self, training_data, eval_data, test_data, optimizer, num_epochs=500, \n",
    "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. You can either train from scratch\n",
    "            or load the latest model trained. Early stopping is used in order to\n",
    "            mitigate the risk of overfitting the network.\n",
    "            \n",
    "            Args:\n",
    "                training_data: the data you would like to train the model on.\n",
    "                                Must be in the tf.data.Dataset format.\n",
    "                eval_data: the data you would like to evaluate the model on.\n",
    "                            Must be in the tf.data.Dataset format.\n",
    "                optimizer: the optimizer used during training.\n",
    "                num_epochs: the maximum number of iterations you would like to \n",
    "                            train the model.\n",
    "                early_stopping_rounds: stop training if the accuracy on the eval \n",
    "                                       dataset does not increase after n epochs.\n",
    "                verbose: int. Specify how often to print the loss value of the network.\n",
    "                train_from_scratch: boolean. Whether to initialize variables of the\n",
    "                                    the last trained model or initialize them\n",
    "                                    randomly.\n",
    "        \"\"\" \n",
    "    \n",
    "        if train_from_scratch==False:\n",
    "            self.restore_model()\n",
    "        \n",
    "        # Initialize best_acc. This variable will store the highest accuracy\n",
    "        # on the eval dataset.\n",
    "        best_acc = 0\n",
    "        \n",
    "        # Initialize classes to update the mean accuracy of train and eval\n",
    "        train_acc = tf.keras.metrics.Accuracy('train_acc')\n",
    "        eval_acc = tf.keras.metrics.Accuracy('eval_acc')\n",
    "        test_acc = tf.keras.metrics.Accuracy('test_acc')\n",
    "        \n",
    "        # Initialize dictionary to store the accuracy history\n",
    "        self.history = {}\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['eval_acc'] = []\n",
    "        self.history['test_acc'] = []\n",
    "        \n",
    "        # Begin training\n",
    "        with tf.device(self.device):\n",
    "            for i in range(num_epochs):\n",
    "                # Training with gradient descent\n",
    "                for step, (X, y, seq_length) in enumerate(training_data):\n",
    "                    grads = self.grads_fn(X, y, seq_length, True)\n",
    "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                # Check accuracy train dataset\n",
    "                for step, (X, y, seq_length) in enumerate(training_data):\n",
    "                    logits = self.predict(X, seq_length, False)\n",
    "                    preds = tf.argmax(logits, axis=1)\n",
    "                    train_acc(preds, y)\n",
    "                self.history['train_acc'].append(train_acc.result().numpy())\n",
    "                # Reset metrics\n",
    "                train_acc.reset_states()\n",
    "\n",
    "                # Check accuracy eval dataset\n",
    "                for step, (X, y, seq_length) in enumerate(eval_data):\n",
    "                    logits = self.predict(X, seq_length, False)\n",
    "                    preds = tf.argmax(logits, axis=1)\n",
    "                    eval_acc(preds, y)\n",
    "                self.history['eval_acc'].append(eval_acc.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_acc.reset_states()\n",
    "                \n",
    "                # Check accuracy test dataset\n",
    "                for step, (X, y, seq_length) in enumerate(test_data):\n",
    "                    logits = self.predict(X, seq_length, False)\n",
    "                    preds = tf.argmax(logits, axis=1)\n",
    "                    test_acc(preds, y)\n",
    "                self.history['test_acc'].append(test_acc.result().numpy())\n",
    "                # Reset metrics\n",
    "                test_acc.reset_states()\n",
    "                \n",
    "                # Print train and eval accuracy\n",
    "                if (i==0) | ((i+1)%verbose==0):\n",
    "                    print('Train accuracy at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
    "                    print('Eval accuracy at epoch %d: ' %(i+1), self.history['eval_acc'][-1])\n",
    "                    print('Test accuracy at epoch %d: ' %(i+1), self.history['test_acc'][-1])\n",
    "\n",
    "                # Check for early stopping\n",
    "                if self.history['eval_acc'][-1]>best_acc:\n",
    "                    best_acc = self.history['eval_acc'][-1]\n",
    "                    count = early_stopping_rounds\n",
    "                else:\n",
    "                    count -= 1\n",
    "                if count==0:\n",
    "                    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926f2014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUDARS~1\\AppData\\Local\\Temp/ipykernel_6336/3603416667.py:45: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  self.rnn_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(cell_size)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save/restore the trained variables.\n",
    "checkpoint_directory = 'models_checkpoints/ImdbRNN/'\n",
    "\n",
    "# Use the GPU if available.\n",
    "device = 'gpu:0'\n",
    "\n",
    "# Define optimizer.\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-6)\n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=1e-6)\n",
    "\n",
    "# Instantiate model. This doesn't initialize the variables yet.\n",
    "lstm_model = RNNModel(vocabulary_size=len(word2idx), device=device, \n",
    "                      checkpoint_directory=checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1216584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sudarshan\\anaconda3\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:754: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "C:\\Users\\Sudarshan\\anaconda3\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:757: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at epoch 1:  0.4966\n",
      "Eval accuracy at epoch 1:  0.4882\n",
      "Test accuracy at epoch 1:  0.49684\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_model.fit(train_dataset, validation_dataset, test_dataset, optimizer, num_epochs=5, \n",
    "                early_stopping_rounds=5, verbose=1, train_from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_model.save_model()\n",
    "checkpoint = tf.train.Checkpoint(lstm_model)\n",
    "save_path = checkpoint.save(checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c86b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer.\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-6)\n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=1e-6)\n",
    "# Instantiate model. This doesn't initialize the variables yet.\n",
    "ugrnn_model = RNNModel(vocabulary_size=len(word2idx), rnn_cell='ugrnn', \n",
    "                       device=device, checkpoint_directory=checkpoint_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67011e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "ugrnn_model.fit(train_dataset, validation_dataset, test_dataset, optimizer, num_epochs=5, \n",
    "                early_stopping_rounds=5, verbose=1, train_from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n",
    "ax1.plot(range(len(lstm_model.history['train_acc'])), lstm_model.history['train_acc'], \n",
    "         label='LSTM Train Accuracy');\n",
    "ax1.plot(range(len(lstm_model.history['eval_acc'])), lstm_model.history['eval_acc'], \n",
    "         label='LSTM Test Accuracy');\n",
    "ax2.plot(range(len(ugrnn_model.history['train_acc'])), ugrnn_model.history['train_acc'],\n",
    "         label='UGRNN Train Accuracy');\n",
    "ax2.plot(range(len(ugrnn_model.history['eval_acc'])), ugrnn_model.history['eval_acc'],\n",
    "         label='UGRNN Test Accuracy');\n",
    "ax1.legend();\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40691f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
